task:
  task_type: training
  project_name: vqvae-train

callbacks:
  LearningRateMonitor:
    logging_interval: step
  EarlyStopping:
    monitor: loss/val
    min_delta: 0.0
    patience: 5
    mode: min

checkpoint:
  auto_insert_metric_name: false
  dirpath: /data/modeling/checkpoints/
  filename: loss={loss/val:.5f}_step={step}
  monitor: loss/val
  save_last: true
  save_top_k: 10
  verbose: true

dataloader:
  num_workers: 4
  batch_size: 1
  pin_memory: True
  persistent_workers: True

data:
  dataset_name: FacesDataset
  common_kwargs:
    base_path: /data/datasets/celeba/images
    train_size: 0.9

trainer:
  accelerator: gpu
  log_every_n_steps: 100
  num_sanity_val_steps: 2
  precision: 16-mixed
  check_val_every_n_epoch: 1
  devices: [0]

model:
    model_name: VQVAE
    model_kwargs:
        im_channels: 3
        model_config:
            z_channels: 8
            codebook_size : 2048
            down_channels : [128, 256, 512, 1024]
            mid_channels : [1024, 1024]
            down_sample : [True, True, True]
            attn_down : [False, False, False]
            norm_channels: 32
            num_heads: 8
            num_down_layers : 2
            num_mid_layers : 3
            num_up_layers : 2
    discriminator_kwargs:
        im_channels: 3
        conv_channels: [128, 256, 512]
    model_lr: 0.00001
    model_betas: [0.5, 0.999]
    disc_lr: 0.00001
    disc_betas: [0.5, 0.999]
    disc_step_start: 15000
    codebook_weight: 1
    commitment_beta: 0.5
    disc_weight: 0.5
    perceptual_weight: 1
    viz_val_indices: [0, 10, 50, 100, 500, 1000]
    viz_train_indices: [0, 10, 50, 100, 500, 1000]
    viz_frequency: 2500
